<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mahmood Sharif</title>
    <link>/authors/michael-k.-reiter/</link>
      <atom:link href="/authors/michael-k.-reiter/index.xml" rel="self" type="application/rss+xml" />
    <description>Mahmood Sharif</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 20 Oct 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu17ec3f4c5dad74ee902e9428b7d65766_36835_512x512_fill_lanczos_center_3.png</url>
      <title>Mahmood Sharif</title>
      <link>/authors/michael-k.-reiter/</link>
    </image>
    
    <item>
      <title>Training Robust ML-based Raw-Binary Malware Detectors in Hours, not Months</title>
      <link>/publication/ccs24-malware-robustness/</link>
      <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
      <guid>/publication/ccs24-malware-robustness/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Group-based Robustness: A General Framework for Customized Robustness in the Real World</title>
      <link>/publication/ndss24-gbr/</link>
      <pubDate>Mon, 26 Feb 2024 00:00:00 +0000</pubDate>
      <guid>/publication/ndss24-gbr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adversarial Training for Raw-Binary Malware Classifiers</title>
      <link>/publication/usenix23-malware-advtrain/</link>
      <pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate>
      <guid>/publication/usenix23-malware-advtrain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Constrained Gradient Descent: A Powerful and Principled Evasion Attack Against Neural Networks</title>
      <link>/publication/icml22-cgd/</link>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      <guid>/publication/icml22-cgd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Malware Makeover: Breaking ML-based Static Analysis by Modifying Executable Bytes</title>
      <link>/publication/asiaccs21-malware/</link>
      <pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate>
      <guid>/publication/asiaccs21-malware/</guid>
      <description></description>
    </item>
    
    <item>
      <title>$n$-ML: Mitigating Adversarial Examples via Ensembles of Topologically Manipulated Classifiers</title>
      <link>/publication/arxiv19-nml/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/publication/arxiv19-nml/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A General Framework for Adversarial Examples with Objectives</title>
      <link>/publication/tops19-adv-ml/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/publication/tops19-adv-ml/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the Suitability of $L_p$-norms for Creating and Preventing Adversarial Examples</title>
      <link>/publication/cvprw18-lpnorm/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>/publication/cvprw18-lpnorm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition</title>
      <link>/publication/ccs16-adv-ml/</link>
      <pubDate>Sat, 01 Oct 2016 00:00:00 +0000</pubDate>
      <guid>/publication/ccs16-adv-ml/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
